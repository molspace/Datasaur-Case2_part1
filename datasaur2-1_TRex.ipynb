{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ntrain_path = '/kaggle/input/datathon-case2-1/case2_1-datasaur/case2_part1_train/train_org.csv'\ntest_path = '/kaggle/input/datathon-case2-1/case2_1-datasaur/org_test_final.csv'\ntest = pd.read_csv(test_path, index_col=0)\nsample_size = 28_672\ntrain = pd.read_csv(train_path, index_col=0)[:sample_size]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-15T10:31:20.050300Z","iopub.execute_input":"2023-10-15T10:31:20.052415Z","iopub.status.idle":"2023-10-15T10:32:17.162076Z","shell.execute_reply.started":"2023-10-15T10:31:20.052274Z","shell.execute_reply":"2023-10-15T10:32:17.160559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport joblib\n\nX_train = train[\"DATA\"].fillna(\"\")\ny_train = train[\"target\"]\nX_test = test[\"DATA\"].fillna(\"\")\n\n# Step 1: Tokenization\nX_train_tokens = X_train.str.split()\nX_test_tokens = X_test.str.split()\n\n# Step 2: Remove Stopwords (optional)\nfrom nltk.corpus import stopwords\n\nstop_words = (\n    set(stopwords.words('english'))\n    .union(set(stopwords.words('russian')))\n    .union(set(stopwords.words('kazakh')))\n)\nX_train_filtered = X_train_tokens.apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\nX_test_filtered = X_test_tokens.apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words])\n\n# Step 3: Apply TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=3000)\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train_filtered.apply(lambda tokens: ' '.join(tokens)))\njoblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\nX_test_tfidf = tfidf_vectorizer.transform(X_test_filtered.apply(lambda tokens: ' '.join(tokens)))\nprint(\"Finished TF-IDF for train and test\")\n\n# Initialize and train a Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=4, random_state=42, n_jobs=-1)\nrf_classifier.fit(X_train_tfidf, y_train)\njoblib.dump(rf_classifier, 'random_forest_classifier.pkl')\nprint(\"Finished model training\")\n\n# Predict on the test data\ny_pred_test = rf_classifier.predict(X_test_tfidf)\n\n# Assign the predictions\ntest[\"target\"] = y_pred_test\n\n# Save to a CSV file\ntest[[\"ID\", \"target\"]].to_csv(\"result_13.csv\", index=False)\nprint(\"Finished\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T10:32:17.164711Z","iopub.execute_input":"2023-10-15T10:32:17.165348Z","iopub.status.idle":"2023-10-15T10:34:47.387448Z","shell.execute_reply.started":"2023-10-15T10:32:17.165308Z","shell.execute_reply":"2023-10-15T10:34:47.386403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')\nloaded_rf_classifier = joblib.load('random_forest_classifier.pkl')\n\nX_test[:1].head()\nloaded_rf_classifier.predict(loaded_tfidf_vectorizer.transform(X_test[:1]))","metadata":{"execution":{"iopub.status.busy":"2023-10-15T11:05:49.277871Z","iopub.execute_input":"2023-10-15T11:05:49.278388Z","iopub.status.idle":"2023-10-15T11:06:22.592170Z","shell.execute_reply.started":"2023-10-15T11:05:49.278354Z","shell.execute_reply":"2023-10-15T11:06:22.590847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"____","metadata":{}},{"cell_type":"code","source":"# Another approach:\n\nimport pandas as pd\n\nmax_n_classes = 8192 // 2\n\ndf = pd.read_csv(train_path, index_col=0).fillna(\"\")\n\n# Get the count of each target value\ntarget_counts = df['target'].value_counts()\n\n# Sort the target values by their counts in descending order\nsorted_target_values = target_counts.index[:max_n_classes]\n\n# Create a new DataFrame to store the selected rows\nselected_df = pd.DataFrame()\n\nfor target_value in sorted_target_values:\n    # Filter rows with the current target value\n    target_rows = df[df['target'] == target_value]\n    \n    # Calculate the number of rows to select for this target value\n    max_rows_per_target = min(sample_size // max_n_classes, len(target_rows))\n    \n    # Select up to max_rows_per_target rows for this target\n    selected_rows = target_rows.head(max_rows_per_target)\n    \n    # Append the selected rows to the new DataFrame\n    selected_df = pd.concat([selected_df, selected_rows], ignore_index=True)\n\n# Now selected_df contains the desired rows with an equal number of rows for each \ntrain = selected_df\ntrain.shape, max_rows_per_target, train[\"target\"].nunique()  # ((32768, 6), 4, 8192)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T07:38:29.756448Z","iopub.execute_input":"2023-10-15T07:38:29.757954Z","iopub.status.idle":"2023-10-15T07:39:34.789033Z","shell.execute_reply.started":"2023-10-15T07:38:29.757912Z","shell.execute_reply":"2023-10-15T07:39:34.787807Z"},"trusted":true},"execution_count":null,"outputs":[]}]}